# Emotion Recognition in Couples using Transfer Learning

## Description

This repository contains scripts to my thesis in emotion recognition in couples using transfer learning. For understanding the context of this work and the contributions, the section [Motivation and contributions](#motivation_contribution) provides an overview. The structure of the repository and the instructions for running the scripts are explained in section [Instructions of running scripts](#instructions). Currently, the thesis is not hosted publicly.

## Motivation and contributions <a name="motivation_contribution"></a>

Emotion recognition in couples has recently received attention from the research community in Computer Science and Psychology. Several studies associate couples' relationships to have an influence on their mental and physical well-being. An indicator of relationship health is the emotions couples feel during their interactions. For instance, couples who have more consistent negative emotions during their interactions are more likely to head toward ending their relationship. Likewise, negative emotions between couples have been associated with health risks. To investigate this relationship-heath link, automatic emotion recognition for couples is proposed by the DyMand project. This project aims to study couples in which one partner manages diabetes. The project conducted a field study which involved real couples. For a week, their interactions were recorded through smartwatches, and their emotions were self-reported using an affective slider. The interaction data collected from smartwatch sensors include accelerometer, heart rate, and audio (which is later transcribed). However, building or re-using an automatic emotion recognition model that will use DyMand's interaction data to predict emotions for couples is non-trivial for many reasons. 

First, the DyMand data is collected from a real-life environment while many emotion recognition models are trained and validated on data collected from unrealistic interactions (i.e., scripted interactions by actors) or the data is collected in ideal environments such as in a lab (which has less noise than real life). Research indicates that re-using models that were trained on ideal environments perform poorly on data from real life environments. Second, the DyMand audio data is in German while many speech emotion recognition models use English language data. Third, the DyMand data is highly imbalanced (as with real life emotion recognition), has limited samples (in the hundreds), which is difficult for machine learning to recognize patterns, and the data is noisy. 

These issues are addressed in this thesis by: creating a deep neural network transfer learning model that uses smartwatch sensor data and German audio data (for issues 1, 2, and 3) and multimodal fusion is explored to leverage multiple modalities (accelerometer, heart rate, acoustic and linguistic) to enhance emotion recognition (for issues 1, and 3). The transfer learning model is trained on two public datasets - Vera-Am-Mittag (for German acoustic and linguistic modalities) and K-EmoCon (for accelerometer and heart rate modalities) and is evaluated on the DyMand dataset using fine-tuning for the respective modalities. Additionally, multimodal fusion approaches of early fusion and late fusion is investigated and compared for the pre-training datasets and the DyMand dataset to understand which modalities can be leveraged and how they can be used together for enhancing emotion recognition for the aforementioned datasets.

## Instructions of running scripts <a name="instructions"></a>

### Overview
There are three folders that contain the scripts of the respective datasets. The public datasets themselves are not hosted in this repository due to permission issues, and the DyMand dataset is not public. The Vera-am-mittag dataset (VAM) is available <a href="https://sail.usc.edu/VAM/vam_info.htm"> here </a> and the paper <a href="https://ieeexplore.ieee.org/document/4607572?arnumber=4607572"> here </a>. The K-EmoCon dataset is available <a href="https://zenodo.org/record/3762962#.Yt_ZTXZBxPY"> here </a> and the paper <a href="https://www.nature.com/articles/s41597-020-00630-y"> here. </a>

### Requirements

The scripts require the following Python packages: scikit-learn, tensorflow, keras, keras-tuner, pandas, numpy, seaborn, time, matplotlib, docx, audiofile, sentencetransformers, opensmile, and scipy.

### Instructions

First run the scripts in the VAM or K-EmoCon folders before running the scripts in the DyMand folder since the former creates the transfer learning models that will be used in the DyMand scripts. For simplicity and re-produceability, the scripts have been combined into one large script. For the VAM folder, run the script in the VAM_Audio_Lexical_Evaluation.ipynb - this contains detailed instructions, early fusion and late fusion approaches and creates the neural network transfer learning model. Similarily, for the K-EmoCon folder, run the script KEmoCon_Merge_Sensor_Evaluation.ipynb for the early fusion and late fusion approaches and also to create the neural network transfer learning model. For the DyMand dataset, there are still multiple scripts due to the high complexity of the dataset. First run the DyMand_Sensors.ipynb script, and the DyMand_Labels.ipynb script for pre-processing, feature extraction and organizing the self-reported emotions. Second, run the DyMand_Audio_Features.ipynb for extracting and organizing the acoustic modality features. Third, run the DyMand_Transcript.ipynb for pre-processing, extracting and organizing the respective transcripts, then run the DyMand Linguistic Features to extract the linguistic features. Finally, run the DyMand_Merge_Sensor_Evaluation.ipynb for evaluating transfer learning and multimodal fusion on the DyMand dataset for accelerometer and heart rate modalities. Similarily, run the DyMand_Merge_Audio_Lexical_Evaluation.ipynb for evaluating transfer learning and multimodal fusion on the DyMand dataset for acoustic and linguistic modalities.
